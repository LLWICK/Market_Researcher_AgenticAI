{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcdfd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9696c58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06eb4685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9342383a0f8344c19a279af035274e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&lt;string&gt;:6: ResourceWarning: unclosed &lt;socket.socket fd=780, family=2, type=1, proto=0, laddr=('127.0.0.1', 13578),\n",
       "raddr=('127.0.0.1', 11434)&gt;\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<string>:6: ResourceWarning: unclosed <socket.socket fd=780, family=2, type=1, proto=0, laddr=('127.0.0.1', 13578),\n",
       "raddr=('127.0.0.1', 11434)>\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&lt;string&gt;:6: ResourceWarning: unclosed &lt;socket.socket fd=1572, family=2, type=1, proto=0, laddr=('127.0.0.1', \n",
       "13971), raddr=('127.0.0.1', 11434)&gt;\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<string>:6: ResourceWarning: unclosed <socket.socket fd=1572, family=2, type=1, proto=0, laddr=('127.0.0.1', \n",
       "13971), raddr=('127.0.0.1', 11434)>\n",
       "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from phi.agent import Agent\n",
    "\n",
    "from phi.model.groq import Groq\n",
    "from phi.tools.duckduckgo import DuckDuckGo\n",
    "from phi.tools.yfinance import YFinanceTools\n",
    "from phi.model.ollama import Ollama\n",
    "from phi.tools.googlesearch import GoogleSearch\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "web_agent = Agent(\n",
    "    name=\"Web Agent\",\n",
    "    #model=Groq(id=\"llama-3.3-70b-versatile\"),\n",
    "    model=Ollama(id=\"llama3.2\"),\n",
    "    tools=[GoogleSearch()],\n",
    "    instructions=[\"Always include sources\"],\n",
    "    show_tool_calls=True,\n",
    "    markdown=True\n",
    ")\n",
    "\n",
    "finance_agent = Agent(\n",
    "    name=\"Finance Agent\",\n",
    "    role=\"Get financial data\",\n",
    "    model=Ollama(id=\"llama3.2\"),\n",
    "    # model=OpenAIChat(id=\"gpt-4o\"),\n",
    "    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],\n",
    "    instructions=[\"Use tables to display data\"],\n",
    "    show_tool_calls=True,\n",
    "    markdown=True,\n",
    ")\n",
    "\n",
    "agent_team = Agent(\n",
    "    model=Groq(id=\"deepseek-r1-distill-llama-70b\"), # give team lean a better model\n",
    "    # model=OpenAIChat(id=\"gpt-4o\"),\n",
    "    team=[web_agent, finance_agent],\n",
    "    instructions=[\"Always include sources\", \"Use tables to display data\"],\n",
    "    show_tool_calls=True,\n",
    "    markdown=True,\n",
    ")\n",
    "\n",
    "res = agent_team.print_response(\"Summarize analyst recommendations and share the latest news for NVDA\", stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "208f86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing IR retrieve code base\n",
    "\n",
    "# agents/data_scraper_ir.py\n",
    "from __future__ import annotations\n",
    "import os, time, json, hashlib, urllib.parse, logging, re, requests\n",
    "from typing import List, Dict, Optional, Iterable\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, HttpUrl, ValidationError\n",
    "from dotenv import load_dotenv\n",
    "import trafilatura\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.fields import Schema, TEXT, ID, DATETIME\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "from whoosh.qparser import MultifieldParser\n",
    "import urllib.robotparser as robotparser\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "BASE = Path(os.getcwd())\n",
    "\n",
    "# Use the / operator to join path components\n",
    "INDEX_DIR = BASE / \"storage\" / \"index\"\n",
    "CACHE_DIR = BASE / \"storage\" / \"cache\"\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(\"DataScraperIR\")\n",
    "\n",
    "UA = \"Mozilla/5.0 DataScraperIR/1.0\"\n",
    "TIMEOUT = 12\n",
    "ALLOWED_SCHEMES = {\"http\", \"https\"}\n",
    "ALLOWED_DOMAINS: Optional[set[str]] = None   # e.g. {\"reuters.com\", \"bloomberg.com\"}\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    title: str\n",
    "    url: HttpUrl\n",
    "    snippet: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "    date: Optional[str] = None\n",
    "\n",
    "class Document(BaseModel):\n",
    "    title: str\n",
    "    url: HttpUrl\n",
    "    content: str\n",
    "    source: Optional[str] = None\n",
    "    published_at: Optional[datetime] = None\n",
    "\n",
    "def _valid_url(u: str) -> bool:\n",
    "    try:\n",
    "        p = urllib.parse.urlparse(u)\n",
    "        if p.scheme not in ALLOWED_SCHEMES: return False\n",
    "        if ALLOWED_DOMAINS:\n",
    "            host = (p.netloc or \"\").lower()\n",
    "            return any(host.endswith(d) for d in ALLOWED_DOMAINS)\n",
    "        return True\n",
    "    except: return False\n",
    "\n",
    "def _robots_ok(u: str) -> bool:\n",
    "    try:\n",
    "        p = urllib.parse.urlparse(u)\n",
    "        rp = robotparser.RobotFileParser()\n",
    "        rp.set_url(f\"{p.scheme}://{p.netloc}/robots.txt\")\n",
    "        rp.read()\n",
    "        return rp.can_fetch(UA, u)\n",
    "    except: return False\n",
    "\n",
    "def _cache_path(u: str) -> str:\n",
    "    h = hashlib.sha256(u.encode()).hexdigest()[:24]\n",
    "    return os.path.join(CACHE_DIR, f\"{h}.json\")\n",
    "\n",
    "def _load_cache(u: str) -> Optional[dict]:\n",
    "    p = _cache_path(u)\n",
    "    return json.load(open(p, \"r\", encoding=\"utf-8\")) if os.path.exists(p) else None\n",
    "\n",
    "def _save_cache(u: str, d: dict) -> None:\n",
    "    json.dump(d, open(_cache_path(u), \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "# ---- Search with Serper (Google Search API)\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(1, 2, 8), reraise=True)\n",
    "def serper_news(query: str, num: int = 10) -> List[SearchResult]:\n",
    "    if not SERPER_API_KEY:\n",
    "        raise RuntimeError(\"SERPER_API_KEY missing\")\n",
    "    r = requests.post(\n",
    "        \"https://google.serper.dev/news\",\n",
    "        headers={\"X-API-KEY\": SERPER_API_KEY, \"Content-Type\": \"application/json\"},\n",
    "        json={\"q\": query, \"num\": min(num, 20)},\n",
    "        timeout=TIMEOUT,\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    out = []\n",
    "    for it in data.get(\"news\", []):\n",
    "        try:\n",
    "            out.append(SearchResult(\n",
    "                title=it.get(\"title\", \"\"),\n",
    "                url=it.get(\"link\", \"\"),\n",
    "                snippet=it.get(\"snippet\"),\n",
    "                source=it.get(\"source\"),\n",
    "                date=it.get(\"date\"),\n",
    "            ))\n",
    "        except ValidationError:\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "# ---- Fetch & extract\n",
    "class ScrapeError(Exception): pass\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(1, 2, 6), reraise=True)\n",
    "def fetch_html(url: str) -> str:\n",
    "    if not _valid_url(url): raise ScrapeError(\"Invalid/disallowed URL\")\n",
    "    if not _robots_ok(url): raise ScrapeError(\"robots.txt disallows\")\n",
    "    r = requests.get(url, headers={\"User-Agent\": UA}, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def extract_text(html: str, url: str) -> Optional[str]:\n",
    "    txt = trafilatura.extract(html, url=url, include_comments=False, include_tables=False)\n",
    "    return txt if txt and len(txt.split()) >= 60 else None\n",
    "\n",
    "def make_doc(url: str, title_hint=None, source=None, date_str=None) -> Optional[Document]:\n",
    "    cached = _load_cache(url)\n",
    "    if cached:\n",
    "        try: return Document(**cached)\n",
    "        except ValidationError: pass\n",
    "    html = fetch_html(url)\n",
    "    content = extract_text(html, url)\n",
    "    if not content: return None\n",
    "    title = title_hint or content.splitlines()[0][:120]\n",
    "    published_at = None\n",
    "    if date_str:\n",
    "        try: published_at = datetime.fromisoformat(re.sub(\"Z$\", \"+00:00\", date_str))\n",
    "        except: pass\n",
    "    doc = Document(title=title, url=url, content=content, source=source, published_at=published_at)\n",
    "    _save_cache(url, json.loads(doc.json()))\n",
    "    return doc\n",
    "\n",
    "# ---- Whoosh index\n",
    "def _ensure_index():\n",
    "    schema = Schema(\n",
    "        url=ID(stored=True, unique=True),\n",
    "        title=TEXT(stored=True, analyzer=StemmingAnalyzer()),\n",
    "        content=TEXT(stored=False, analyzer=StemmingAnalyzer()),\n",
    "        source=TEXT(stored=True),\n",
    "        published_at=DATETIME(stored=True),\n",
    "    )\n",
    "    if not os.listdir(INDEX_DIR):\n",
    "        return create_in(INDEX_DIR, schema)\n",
    "    return open_dir(INDEX_DIR)\n",
    "\n",
    "def index_docs(docs: Iterable[Document]) -> int:\n",
    "    ix = _ensure_index()\n",
    "    w = ix.writer(limitmb=128)\n",
    "    n = 0\n",
    "    for d in docs:\n",
    "        w.update_document(\n",
    "            url=str(d.url),\n",
    "            title=d.title,\n",
    "            content=d.content,\n",
    "            source=d.source or \"\",\n",
    "            published_at=d.published_at\n",
    "        ); n += 1\n",
    "    w.commit()\n",
    "    return n\n",
    "\n",
    "def ir_search(query: str, limit: int = 10) -> List[Dict]:\n",
    "    ix = _ensure_index()\n",
    "    with ix.searcher() as s:\n",
    "        q = MultifieldParser([\"title\", \"content\"], ix.schema).parse(query)\n",
    "        rs = s.search(q, limit=limit)\n",
    "        return [{\n",
    "            \"title\": r.get(\"title\"),\n",
    "            \"url\": r.get(\"url\"),\n",
    "            \"source\": r.get(\"source\"),\n",
    "            \"published_at\": r.get(\"published_at\").isoformat() if r.get(\"published_at\") else None,\n",
    "            \"score\": float(r.score),\n",
    "        } for r in rs]\n",
    "\n",
    "# ---- Orchestrate: search -> scrape -> index\n",
    "def collect_and_index(query: str, k_search: int = 10, k_index: int = 8) -> Dict:\n",
    "    results = serper_news(query, num=k_search)\n",
    "    docs = []\n",
    "    for it in results[:k_index]:\n",
    "        try:\n",
    "            d = make_doc(str(it.url), title_hint=it.title, source=it.source, date_str=it.date)\n",
    "            if d:\n",
    "                docs.append(d)\n",
    "                time.sleep(1.0)  # be polite\n",
    "        except Exception as e:\n",
    "            log.warning(f\"Skip {it.url}: {e}\")\n",
    "    n = index_docs(docs)\n",
    "    return {\"indexed\": n, \"query\": query, \"examples\": [d.title for d in docs[:5]]}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market-researcher-agenticai (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
